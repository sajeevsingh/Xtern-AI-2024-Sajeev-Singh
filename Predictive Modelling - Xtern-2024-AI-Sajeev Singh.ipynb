{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f36b1d9b",
   "metadata": {},
   "source": [
    "#### Let us now try fitting the various different models in this data to find which one fits best. We will also be using ensembling techniques like Random Forest to see if that fits well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d617be4",
   "metadata": {},
   "source": [
    "#### We need to convert the data into numerical format so that our algorithms can work on it, Then we need to split the data into training and testing data and then we need to test our model over the test data and see the scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd35781b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "import joblib\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70686c64",
   "metadata": {},
   "source": [
    "#### Converting data into format that can be fed into machine learning algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "974f6127",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\sajee\\OneDrive\\Desktop\\Xtern AI- Sajeev Singh- 2024\\Data.csv\")\n",
    "df2 = pd.read_csv(r\"C:\\Users\\sajee\\OneDrive\\Desktop\\Xtern AI- Sajeev Singh- 2024\\Menu.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8ea3315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Year  Major  University  Time  Order\n",
      "0     2      0           0    12      0\n",
      "1     3      1           1    14      1\n",
      "2     3      1           2    12      2\n",
      "3     2      2           0    11      0\n",
      "4     3      3           2    12      3\n"
     ]
    }
   ],
   "source": [
    "copy = df.copy()\n",
    "mappings = {\n",
    "    'Year': {'Year 1': 1, 'Year 2': 2, 'Year 3': 3, 'Year 4': 4},\n",
    "    'Major': {value: index for index, value in enumerate(df['Major'].unique())},\n",
    "    'University': {value: index for index, value in enumerate(df['University'].unique())},\n",
    "    'Order': {value: index for index, value in enumerate(df['Order'].unique())}\n",
    "}\n",
    "for column, mapping in mappings.items():\n",
    "    copy[column] = copy[column].map(mapping)\n",
    "\n",
    "print(copy.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9f96d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = copy[['University', 'Major', 'Year', 'Time']] ### Setting X-variables\n",
    "y = copy['Order'] ### Setting target variable as y\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17d29c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using sklearn test train split and splitting the data in normal 20-80.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=20, shuffle= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd06d14e",
   "metadata": {},
   "source": [
    "#### Now we will use the different algorithms to find the best one in these\n",
    "#### 1. Random Forest Classifier\n",
    "#### 2. K- Nearest Neighbours\n",
    "#### 3. SVC\n",
    "#### 4. Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cccf51b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'precision_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_27548\\84251502.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0maccuracy_scores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mprecision_scores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprecision_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'weighted'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Use weighted average for multiclass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0mrecall_scores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecall_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'weighted'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Use weighted average for multiclass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mf1_scores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'weighted'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Use weighted average for multiclass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'precision_score' is not defined"
     ]
    }
   ],
   "source": [
    "classifiers = [\n",
    "    ('Random Forest', RandomForestClassifier(random_state=0)),\n",
    "    ('K-Nearest Neighbors', KNeighborsClassifier()),\n",
    "    ('Support Vector Machine', SVC()),\n",
    "    ('Decision Tree', DecisionTreeClassifier(random_state=0))\n",
    "]\n",
    "\n",
    "# Initializing lists to store evaluation metrics\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "for name, classifier in classifiers:\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
    "    precision_scores.append(precision_score(y_test, y_pred, average='weighted'))  # Use weighted average for multiclass\n",
    "    recall_scores.append(recall_score(y_test, y_pred, average='weighted'))  # Use weighted average for multiclass\n",
    "    f1_scores.append(f1_score(y_test, y_pred, average='weighted'))  # Use weighted average for multiclass\n",
    "\n",
    "import pandas as pd\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Classifier': [name for name, _ in classifiers],\n",
    "    'Accuracy': accuracy_scores,\n",
    "    'Precision': precision_scores,\n",
    "    'Recall': recall_scores,\n",
    "    'F1-Score': f1_scores\n",
    "})\n",
    "\n",
    "### Creating a bar plot for metrics\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Classifier', y='Accuracy', data=metrics_df, palette='viridis')\n",
    "plt.title('Classifier Performance Comparison (Accuracy)')\n",
    "plt.xlabel('Classifier')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "print(metrics_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea194221",
   "metadata": {},
   "source": [
    "#### Clearly Decision Tree and Random Forest works best in this case and thus I choose the Decision Tree classifier as my final model because of less complexity in that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f374d9",
   "metadata": {},
   "source": [
    "#### We can also use advance techniques of combining ML algo and tuning it but that would not give any extra result in this case as there are no such patterns already found here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bc1bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(d_tree, 'decision_tree_model.pkl')\n",
    "loaded_model = joblib.load('decision_tree_model.pkl')\n",
    "### Now we can use 'loaded_model' for predictions\n",
    "y_pred = loaded_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b579e5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_m = confusion_matrix(y_test, y_pred)\n",
    "print(confusion_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9a6134",
   "metadata": {},
   "source": [
    "## Answer to last question- \n",
    "#### To assess the viability of launching a FoodX food recommender AI, key considerations include data quality, model performance, user interest, and cost-effectiveness. Collecting and maintaining data can be resource-intensive, so ensuring data cleanliness and unbiased information is essential. The AI model should consistently outperform random guessing to justify its implementation. User engagement and interest in such a system should be evaluated. A cost-benefit analysis, comparing potential savings from offering discounts through AI predictions versus traditional methods, is crucial. Addressing the current lack of discernible patterns in the data involves gathering more data and expanding measured variables. Investing in stronger models, like XGBoost, can enhance predictive capabilities. Time sensitivity plays a role, as quick deployment may not align with data readiness. Balancing these factors is key to determining the feasibility of launching the FoodX AI recommender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d329a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
